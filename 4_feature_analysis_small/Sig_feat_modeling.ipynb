{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOAL: <br>\n",
    "to find the stat sig features and then use these to train a model... <br>\n",
    "need to find stat sig features inside the train / val / test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART OF THIS CODE IS FROM MICHAEL MURPHY - THANKS!\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import glob, re\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_rows', 500) \n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.multiclass import type_of_target # used to check the Y labels are appropriate for classification\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import interp\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import kruskal\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "\n",
    "def get_num_labels(ds):\n",
    "    ds['labels'] = ds['labels']*1\n",
    "    vals = ds['labels'].values\n",
    "    try:\n",
    "        vals = [item for sublist in vals for item in sublist]\n",
    "    except:\n",
    "        pass\n",
    "    labels = set(vals)\n",
    "    ds['num_labels'] = len(labels)\n",
    "    ds['label_set'] = labels\n",
    "    return ds\n",
    "\n",
    "def check_pre_norm(ds):\n",
    "    if ds['data_set'] in pre_norm_ds:\n",
    "        ds['pre_norm'] = 'Yes'\n",
    "    else:\n",
    "        ds['pre_norm'] = 'No'\n",
    "    return ds\n",
    "\n",
    "def convert_nan_to_val(data, value=0):\n",
    "    data[pd.isnull(data)] = value\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Use this if DOING a fresh modeling fitting analysis\n",
    "bn = True # use the percentile normalized data or no? \n",
    "log = False\n",
    "stand_scaler = False\n",
    "reanalysis = True\n",
    "model = 'log_reg' #log_reg, rf or svm\n",
    "\n",
    "if reanalysis:\n",
    "    pre_norm_ds = [ 'plasmaall_author',\n",
    "                    'urineall_author',\n",
    "                    'm_oxylipin_chronic_hep_b',\n",
    "                    'm_chronic_hep_b_POS',\n",
    "                    'm_chronic_hep_b_NEG',\n",
    "                    'm_CER_mass_spectrometry_v4',\n",
    "                    'm_CER_mass_spectrometry_v4_3_CS',\n",
    "                    'm_CER_mass_spectrometry_v4_0_NS',\n",
    "                    'm_CER_mass_spectrometry_v4_2_FS',\n",
    "                    'm_CER_mass_spectrometry_v4_1_COPD',\n",
    "                    'm_EICO_mass_spectrometry_v4',\n",
    "                    'm_EICO_mass_spectrometry_v4_3_CS',\n",
    "                    'm_EICO_mass_spectrometry_v4_0_NS',\n",
    "                    'm_EICO_mass_spectrometry_v4_2_FS',\n",
    "                    'm_EICO_mass_spectrometry_v4_1_COPD',\n",
    "                    'AN000580',\n",
    "                    'AN000581',\n",
    "                    'AN001503',\n",
    "                    'ulsam_author']\n",
    "\n",
    "    if bn:\n",
    "        path = './bn_pickles/*.pkl'\n",
    "    else:\n",
    "        path = './pickles/*.pkl'\n",
    "\n",
    "    datasets = OrderedDict()\n",
    "    for fn in sorted(glob.glob(path)):\n",
    "        data = pd.read_pickle(open(fn,'rb'))\n",
    "        datasets[data[0]['study']] = data\n",
    "    \n",
    "else:\n",
    "    #### Use this if NOT doing a fresh modeling fitting analysis\n",
    "    pickle_file = './YES_bn_ds_models_and_sigfeat_NO_log_NO_standscal_NO_multi_mapped_labels.pkl'\n",
    "    ### The non-batch corrected pickle for the dataset\n",
    "    # pickle_file = './NO_bn_dataset_models_and_sigfeat_YES_log.pkl'\n",
    "    datasets = pickle.load(open(pickle_file, 'rb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X,y,ds,model):\n",
    "    X,y = shuffle(X,y)\n",
    "    if model == 'log_reg':\n",
    "        if ds['num_labels'] != 2:\n",
    "            clf = LogisticRegressionCV(scoring='accuracy', penalty='l1', solver='liblinear', tol=1e-4, intercept_scaling=1, max_iter=500, multi_class='ovr')\n",
    "        else:\n",
    "            clf = LogisticRegressionCV(scoring='roc_auc', penalty='l1', solver='liblinear', tol=1e-4, intercept_scaling=1, max_iter=500)\n",
    "    elif model == 'rf':\n",
    "        param_grid = {'n_estimators':[100,500,1000]}\n",
    "        clf = GridSearchCV(RandomForestClassifier(n_estimators=1000, n_jobs=-1), param_grid, cv=3, n_jobs=-1)\n",
    "    elif model == 'svm':\n",
    "        param_grid = {'gamma': [1e-3, 0.01, 0.1, 1], 'C': [0.01, 0.1, 1, 10, 100]}\n",
    "        clf = GridSearchCV(SVC(kernel='linear', probability=True), param_grid, cv=3, n_jobs=-1)\n",
    "    elif model == 'plsda':\n",
    "        param_grid = {'n_components': [2,3,5,10,50,100]}\n",
    "        clf = GridSearchCV(PLSRegression(), param_grid, cv=3, n_jobs=-1)\n",
    "    else:\n",
    "        print('no valid classifier input, please try again with one of: log_reg, rf or svm')\n",
    "        exit(0)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True) # so this will probably give rather high - at the end you just get the last model...\n",
    "    aucs = []\n",
    "    num_stat = []\n",
    "    for train, test in cv.split(X,y):\n",
    "        x_train, y_train = X[train], y[train]\n",
    "        x_test, y_test = X[test], y[test]\n",
    "#         print('train, test pre: ',x_train.shape, x_test.shape)\n",
    "        ###### i guess here you can find the sig features using the train data, and mask the x_train / then x_test \n",
    "        p = np.zeros(x_train.shape[1]) + np.nan\n",
    "        for i in range(x_train.shape[1]):\n",
    "            feat_data = []\n",
    "            for j in set(y_train):\n",
    "                try:\n",
    "                    X_0 = x_train[y_train==j,i]\n",
    "                    X_0 = X_0[~np.isnan(X_0)]\n",
    "                    feat_data.append(X_0)\n",
    "                except:\n",
    "                    pass \n",
    "            if set(feat_data[0]) == set(feat_data[1]):\n",
    "                p[i] = 1\n",
    "                continue\n",
    "            else:\n",
    "                _, p[i] = mannwhitneyu(feat_data[0],feat_data[1], alternative='two-sided')                     \n",
    "        try:\n",
    "            _, p[~np.isnan(p)], _, _ = multipletests(p[~np.isnan(p)], alpha=0.05, method='fdr_bh')\n",
    "        except:\n",
    "            pass\n",
    "        x_train = x_train[:,p<0.05]\n",
    "        x_test = x_test[:,p<0.05]\n",
    "        num_stat.append(x_train.shape[1])\n",
    "        if x_train.shape[1] == 0:\n",
    "            return 0.5, 0, ',', y_train.shape, y_test.shape, 0, np.asarray(num_stat).mean()\n",
    "        if x_test.shape[1] == 0:\n",
    "            return 0.5, 0, ',', y_train.shape, y_test.shape, 0, np.asarray(num_stat).mean()\n",
    "#         print('train, test post: ', x_train.shape, x_test.shape)\n",
    "        if stand_scaler:\n",
    "            scaler = StandardScaler()\n",
    "            x_train = scaler.fit_transform(x_train)\n",
    "            x_test = scaler.transform(x_test)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict_proba(x_test)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred[:,1])\n",
    "        auc_value = metrics.auc(fpr, tpr)\n",
    "        aucs.append(auc_value) \n",
    "    auc = np.asarray(aucs)\n",
    "    if ds['num_labels'] != 2:\n",
    "        multi_aucs = auc\n",
    "    else: multi_aucs = 0\n",
    "    return auc.mean(), auc.std(), clf, y_train.shape, y_test.shape, multi_aucs, np.asarray(num_stat).mean()\n",
    "\n",
    "def fit_model(X,y,ds,model):\n",
    "    mean, std, clf, train_size, test_size, multi_aucs, num_stat =  train_model(X,y,ds,model)\n",
    "    if mean == 1.0 or mean == 0.5:\n",
    "        mean, std, clf, train_size, test_size, multi_aucs, num_stat = train_model(X,y,ds,model)\n",
    "    return mean, std, train_size[0], test_size[0], clf, multi_aucs, num_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feng plasmaall_author (102, 109)\n"
     ]
    }
   ],
   "source": [
    "# Used to fit models for all the datasets!\n",
    "for k, v in datasets.items():  \n",
    "    for ds in v:   \n",
    "        print(k, ds['data_set'], ds['features'].shape)\n",
    "        ds = get_num_labels(ds)\n",
    "        ds = check_pre_norm(ds)                \n",
    "        y = ds['labels'].values.copy().ravel().astype(int)    \n",
    "        X = ds['features'].values.copy()\n",
    "        X = convert_nan_to_val(X, value=0)\n",
    "        X[np.isinf(X)] = 0\n",
    "        X[X<0] = 0\n",
    "        if log and ds['pre_norm'] == 'No':\n",
    "            print('using log')\n",
    "            X[X<1] = 1\n",
    "            X = np.log2(X)\n",
    "        aucs = []\n",
    "        avg_stat_sig = []\n",
    "        for i in range(30):\n",
    "            auc, std,train_size,test_size,clf,multi_auc, num_stat = fit_model(X,y,ds,model)\n",
    "            aucs.append(auc)\n",
    "            avg_stat_sig.append(num_stat)\n",
    "        ds['avg_stat_sig'] = np.asarray(avg_stat_sig).mean()\n",
    "        aucs = np.asarray(aucs)\n",
    "        ds['auc'] = aucs.mean()\n",
    "        ds['auc_std'] = aucs.std()\n",
    "        ds['train_size'], ds['test_size'], ds['clf'], ds['multi_aucs'] = train_size, test_size, clf, multi_auc\n",
    "#         ds['auc'], ds['auc_std'], ds['train_size'], ds['test_size'], ds['clf'], ds['multi_aucs'] = fit_model(X, y, ds, model=model)     \n",
    "        print(ds['auc'],ds['auc_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_type = {\n",
    "    'acute myocardial infarction': 'cardiovascular',\n",
    "    'cardiovascular': 'cardiovascular',\n",
    "    'coronary heart disease': 'cardiovascular',\n",
    "    'hepatocellular carcinoma': 'cancer',\n",
    "    'Hepatocellular carcinoma': 'cancer',\n",
    "    'Hepatocellular Carcinoma': 'cancer',\n",
    "    'hepatitis b': 'infectious',\n",
    "    'Malaria': 'infectious',\n",
    "    'Malaria (P. vivax)':'infectious',\n",
    "    'non-malaria febrile illness':'infectious',\n",
    "    'scleroderma PAH': 'autoimmune',\n",
    "    'psoriasis':'autoimmune',\n",
    "    'pneumonia': 'infectious',\n",
    "    'Pneumonia - Community acquired': 'infectious',\n",
    "    'copd': 'respiratory',\n",
    "    'COPD': 'respiratory',\n",
    "    'chronic hepatitis B' : 'infectious',\n",
    "    'typhoid': 'infectious',\n",
    "    'typhoid carriage':'infectious',\n",
    "    'lyme': 'infectious',\n",
    "    'common cold - longitudinal':'infectious',\n",
    "    'Lyme disease': 'infectious',\n",
    "    'Alzheimers': 'neurological',\n",
    "    \"Alzheimer's\": 'neurological',\n",
    "    'colorectal cancer': 'cancer',\n",
    "    'Colorectal Cancer': 'cancer',\n",
    "    'depression': 'neurological',\n",
    "    'Depression':'neurological',\n",
    "    'Breast Cancer': 'cancer',\n",
    "    'Breast cancer':'cancer',\n",
    "    'Lung cancer': 'cancer',\n",
    "    'lung cancer': 'cancer',\n",
    "    'Lung Cancer': 'cancer',\n",
    "    'lung cancer - adenocarcinoma': 'cancer',\n",
    "    'lung cancer - non-small-cell lung cancer (adenocarcinoma, etc)': 'cancer',\n",
    "    'Stability of dried blood samples - diabetic men' : 'metabolic',\n",
    "    'Obesity - Non-diabetic and T2 diabetic': 'metabolic',\n",
    "    't2 diabetes': 'metabolic',\n",
    "    't1 diabetes': 'metabolic',\n",
    "    'Diabetes - Type I': 'metabolic',\n",
    "    'Diabetes - healthy v. T2 v. prediabetic': 'metabolic',\n",
    "    'Polycystic Ovarian Syndrome': 'metabolic',\n",
    "    'minimal change disease, focal segmental sclerosis': 'glomerular',\n",
    "    'interstitial cystitis/painful bladder syndrome': 'other',\n",
    "    'prepubertal children with obesity': 'other', #MAYBE CHANGE THIS ONE?\n",
    "    'chronic fatigue syndrome': 'other',\n",
    "    'Chronic fatigue': 'other',\n",
    "    'polycystic ovarian syndrome': 'other',\n",
    "    'scleroderma': 'other',\n",
    "    'Pregnancy': 'other',\n",
    "    'smoker v. nonsmoker':'other',\n",
    "    'Interperson variation':'other',\n",
    "    'short-term and long-term metabolic changes after bariatric surgery':'other',\n",
    "    'high intensity exercise metabolomics':'other',\n",
    "    'Age related metabolomics': 'other',\n",
    "    'Urine sample storage': 'other',\n",
    "    'urine metabolome': 'other',\n",
    "    'Single human time study': 'other'\n",
    "    }\n",
    "\n",
    "def make_summary(u,l,i,k,j=0, replace=False):\n",
    "    if replace:\n",
    "        auc = u['multi_aucs'].mean(0)[j]\n",
    "        auc_std = u['multi_aucs'].std(0)[j]\n",
    "        analysis = u['data_set']+'_'+str(j)\n",
    "        label = str(l)+str(i)+str(j)\n",
    "        if model == 'log_reg':\n",
    "            model_coef = np.count_nonzero(u['clf'].coef_[j])\n",
    "    else:\n",
    "        auc = u['auc']\n",
    "        auc_std = u['auc_std']\n",
    "        analysis = u['data_set']\n",
    "#         print(str(l), str(i))\n",
    "        label = str(l)+str(i)\n",
    "#         if model == 'log_reg':\n",
    "#             print(u['clf'])\n",
    "#             model_coef = np.count_nonzero(u['clf'].coef_)\n",
    "    s = {'disease': u['disease'], \n",
    "        'number_labels': 2,\n",
    "        'auc':auc,\n",
    "        'auc_std': auc_std,\n",
    "        'stat_sig_feat':u['avg_stat_sig'],\n",
    "        'label': label,\n",
    "        'analysis': analysis,\n",
    "        'disease_type': disease_type[u['disease']],\n",
    "        'study': k}\n",
    "#     if model == 'log_reg':\n",
    "#         s['model_nonzero_coef'] = model_coef\n",
    "    return s\n",
    "    \n",
    "from string import ascii_letters\n",
    "summary = []\n",
    "for l,k in zip(ascii_letters, datasets):\n",
    "    for i, u in enumerate(datasets[k]):\n",
    "        if (k == 'ST000062' and u['data_set'] == 'XCMS-Report-annotated-SingleClass-GCTOF.'):\n",
    "            u['data_set'] = 'XCMS-Report-annotated-SingleClass-GCTOF.plasma'\n",
    "        if u['num_labels'] == 2:\n",
    "            control = u['labels']==0\n",
    "            case = u['labels']==1\n",
    "            try:\n",
    "                summed_control = int(control.sum())\n",
    "                summed_case = int(case.sum())\n",
    "            except:\n",
    "                pass\n",
    "            summary.append(make_summary(u,l,i,k))\n",
    "        else:\n",
    "            for j in range(u['num_labels']):\n",
    "                summary.append(make_summary(u,l,i,k,j=j,replace=True))\n",
    "                #### Issues with multi class: the case and control just use the last datasets values, not real                      \n",
    "summary = pd.DataFrame(summary)\n",
    "# summary = summary.set_index('study')\n",
    "# summary['disease_type'] = summary['disease_type'].astype('category')\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the df as a csv:\n",
    "summary.to_csv('./onlysigfeat_withnumbers_30avg_auc_{}_sigfeat_summary_YES_bn_NO_log_NO_standscal_YES_ovo.csv'.format(model))\n",
    "# save the df as a pickle\n",
    "summary.to_pickle('./onlysigfeat_withnumbers_30avg_auc_{}_sigfeat_df_YES_bn_NO_log_NO_standscal_YES_ovo.pkl'.format(model))\n",
    "# save dataset object:\n",
    "pickle.dump(datasets, open('./onlysigfeat_withnumbers_30avg_{}_YES_bn_ds_models_and_sigfeat_NO_log_NO_standscal_YES_ovo.pkl'.format(model), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the extra metadata onto this data (column, mode, sample type)\n",
    "# metadata = pd.read_csv('./ms_instrument_column_polarity_dataset_names.csv', sep='\\t')\n",
    "metadata = pd.read_csv('./ms_instrument_column_polarity_dataset_names.csv', sep='\\t').set_index('Accession')\n",
    "summary_w_metadata = summary.merge(metadata, on='analysis')\n",
    "summary_w_metadata = summary_w_metadata.replace(np.nan,'unknown')\n",
    "summary_w_metadata.to_csv('./1onlysigfeat_withnumbers_30avg_{}_auc_sigfeat_summary_YES_bn_NO_log_NO_standscal_YES_ovo_YES_meta.csv'.format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
