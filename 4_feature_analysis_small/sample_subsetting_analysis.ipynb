{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to look at the effect of filtering the samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART OF THIS CODE IS FROM MICHAEL MURPHY - THANKS!\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows', 500) \n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.utils.multiclass import type_of_target # used to check the Y labels are appropriate for classification\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot(xs, ys, ss, cs, ls, sscale=1, sbins=None):\n",
    "    ax = plt.gca()\n",
    "    cm = plt.get_cmap('rainbow')\n",
    "    for i, c in enumerate(cs.unique()):\n",
    "        ax.scatter(xs[cs==c],\n",
    "                    ys[cs==c],\n",
    "                    s=ss*sscale if isinstance(ss,int) else ss[cs==c]*sscale,\n",
    "                    c=cm(1.*i/len(cs.unique())),\n",
    "                    edgecolor='k',\n",
    "                    alpha=0.9,\n",
    "                    vmin=0, vmax=len(cs.unique()),\n",
    "                   label='_nolegend_')\n",
    "        ax.scatter([],[],c=cm(1.*i/len(cs.unique())),edgecolor='k',label=c)\n",
    "    ax.scatter([],[],marker='None',label=' ')\n",
    "    if not isinstance(ss,int):\n",
    "        for s in sbins:\n",
    "            ax.scatter([],[],c='k',edgecolor='k',s=s*sscale,label=str(s))\n",
    "            ax.scatter([],[],marker='None',label=' ')\n",
    "    for x, y, l in zip(xs, ys, ls):\n",
    "        ax.text(x, y, l, color='k', ha='center', va='center')\n",
    "    ax.set_xlabel(xs.name)\n",
    "    ax.set_ylabel(ys.name)\n",
    "    ax.legend()\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = True\n",
    "if bn:\n",
    "    path = './bn_pickles/*.pkl'\n",
    "else:\n",
    "    path = './pickles/*.pkl'\n",
    "\n",
    "datasets = OrderedDict()\n",
    "for fn in sorted(glob.glob(path)):\n",
    "    data = pd.read_pickle(open(fn,'rb'))\n",
    "    datasets[data[0]['study']] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_nan_to_val(data, value=0):\n",
    "    data[pd.isnull(data)] = value\n",
    "    return data\n",
    "\n",
    "def extract_random_subset(X, y, percent):\n",
    "    num_to_keep = int(X.shape[0]*percent)\n",
    "    idx = np.random.randint(X.shape[0], size=num_to_keep)\n",
    "    return X[idx,:], y[idx,:].ravel()\n",
    "\n",
    "def l1_log_reg(X,y,ds):\n",
    "    X,y = shuffle(X,y)\n",
    "    # intercept scaling of 1 seems to help with the 0.5 AUC, think it was a convergence issues? \n",
    "    clf = LogisticRegressionCV(scoring='roc_auc', penalty='l1', solver='liblinear', tol=1e-4, intercept_scaling=1, max_iter=500)\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    aucs = []\n",
    "    for train, test in cv.split(X,y):\n",
    "        x_train, y_train = X[train], y[train]\n",
    "        x_test,y_test = X[test], y[test]\n",
    "        scaler = StandardScaler()\n",
    "        x_train = scaler.fit_transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        clf.fit(x_train, y_train)\n",
    "        if ds['num_labels'] != 2:\n",
    "            aucs.append(clf.score(x_test, y_test))\n",
    "        else:\n",
    "            y_pred = clf.predict_proba(x_test)\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred[:,1])\n",
    "            auc_value = metrics.auc(fpr, tpr)\n",
    "            aucs.append(auc_value)            \n",
    "    auc = np.asarray(aucs)\n",
    "    # found sometimes that something will just go wrong in fitting and it will shut down all features and give a 0.5 model\n",
    "    # also sometimes the 1.0 models are fit wrong...\n",
    "    if auc.mean() == 1.0 or auc.mean() == 0.5:\n",
    "        X,y = shuffle(X,y)\n",
    "        clf = LogisticRegressionCV(penalty='l1', scoring='roc_auc', solver='liblinear', tol=1e-4, intercept_scaling=1, max_iter=500)\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "        aucs = []\n",
    "        for train, test in cv.split(X,y):\n",
    "            x_train, y_train = X[train], y[train]\n",
    "            x_test,y_test = X[test], y[test]\n",
    "            scaler = StandardScaler()\n",
    "            x_train = scaler.fit_transform(x_train)\n",
    "            x_test = scaler.transform(x_test)\n",
    "            clf.fit(x_train, y_train)\n",
    "            if ds['num_labels'] != 2:\n",
    "                aucs.append(clf.score(x_test, y_test))\n",
    "            else:\n",
    "                y_pred = clf.predict_proba(x_test)\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_pred[:,1])\n",
    "                auc_value = metrics.auc(fpr, tpr)\n",
    "                aucs.append(auc_value)  \n",
    "    return auc.mean(), auc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feng plasmaall_author\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "Feng urineall_author\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "Feng serum_IPO_aligned_Feng_serum_batch1\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feng serum_IPO_aligned_Feng_serum_batch2\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "Feng urine_IPO_aligned_Feng_urine_batch1\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for k, v in datasets.items():  \n",
    "#     if k not in  ['ST000763', 'ST000865', 'ST000888', 'ST000918']:\n",
    "#         continue\n",
    "    plt.figure(figsize=(8,5))\n",
    "    for ds in v:\n",
    "#         if k in ['MTBLS148','MTBLS200', 'MTBLS20', 'ST000397', 'MTBLS264', 'snyder']:\n",
    "#             ds['p_auc'] = []\n",
    "#             ds['p_std'] = [] # use len(ds['p_auc']) to pass these during plotting\n",
    "#             continue\n",
    "#         if 'XCMS' not in ds['data_set']:\n",
    "#             continue\n",
    "            \n",
    "        ds['labels'] = ds['labels']*1\n",
    "        try:\n",
    "            vals = ds['labels'].values\n",
    "        except:\n",
    "            vals = ds['labels']\n",
    "        try:\n",
    "            vals = [item for sublist in vals for item in sublist]\n",
    "        except:\n",
    "            pass\n",
    "        labels = set(vals)\n",
    "        ds['num_labels'] = len(labels)\n",
    "\n",
    "#         if ds['data_set'] == 'IPO_aligned_MTBLS92':\n",
    "#             f = [fi for fi in list(ds['features'].index) if '163' in fi]\n",
    "#             ds['features'] = ds['features'].loc[f]\n",
    "#             ds['labels'] = ds['labels'].loc[f]\n",
    "#         if ds['study'] == 'MTBLS92' and ds['data_set'] == 'Author_data':\n",
    "#             f = [fi for fi in list(ds['features'].index) if 'A' in fi]\n",
    "#             ds['features'] = ds['features'].loc[f]\n",
    "#             ds['labels'] = ds['labels'].loc[f]\n",
    "        print(k, ds['data_set'])    \n",
    "        \n",
    "        \n",
    "        if bn:\n",
    "            try:\n",
    "                y = ds['labels'].values.copy().astype(int)\n",
    "                X = ds['features']\n",
    "            except:\n",
    "                y = ds['labels']\n",
    "                X = ds['features']\n",
    "        else:\n",
    "            X = ds['features'].values.copy()\n",
    "            y = ds['labels'].values.copy().astype(int)\n",
    "        y = np.reshape(y, (-1,1))\n",
    "        X = convert_nan_to_val(X, value=0)\n",
    "        X[np.isinf(X)] = 0\n",
    "        # for log transform:\n",
    "#         if 'IPO' in ds['data_set'] or 'XCMS' in ds['data_set']:\n",
    "#             X[X==0] = 1\n",
    "#             X = np.log2(X)\n",
    "        #define the number of repeats for each level of subsetting\n",
    "        num_repeats = 3\n",
    "        percents = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1] # levels of subsetting\n",
    "        ds['p_auc'] = []\n",
    "        ds['p_std'] = []\n",
    "        for p in percents:\n",
    "            results = []\n",
    "            for i in range(num_repeats):\n",
    "                X_sub,y_sub = extract_random_subset(X,y, percent=p)\n",
    "                if X_sub.shape[0] < 10:\n",
    "                    results.append(0)\n",
    "                    continue                    \n",
    "                try:\n",
    "                    auc_mean, auc_std = l1_log_reg(X_sub,y_sub,ds)\n",
    "                    results.append(auc_mean)\n",
    "                except:\n",
    "                    results.append(np.nan)\n",
    "            results = np.asarray(results)\n",
    "            mean = np.nanmean(results)\n",
    "            std = np.nanstd(results)\n",
    "            ds['p_auc'].append(mean)\n",
    "            ds['p_std'].append(std)  \n",
    "#         plt.plot(percents, ds['p_auc'], label=ds['data_set'])\n",
    "        plt.errorbar(percents, ds['p_auc'], ds['p_std'], marker='.', label=ds['data_set']+' '+str(X.shape[1]))\n",
    "#         plt.legend(bbox_to_anchor=(1, 0.5))\n",
    "        plt.legend()\n",
    "        plt.xlabel('Fraction of Samples')\n",
    "        plt.ylabel('AUC')  \n",
    "        plt.title(k)\n",
    "#     plt.tight_layout()\n",
    "    plt.savefig(k)\n",
    "    plt.gcf().clear()\n",
    "#         print(ds['p_auc'], ds['p_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
