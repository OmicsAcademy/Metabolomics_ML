{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART OF THIS CODE IS FROM MICHAEL MURPHY - THANKS!\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import glob, re\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_rows', 500) \n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.multiclass import type_of_target # used to check the Y labels are appropriate for classification\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import interp\n",
    "\n",
    "def get_num_labels(ds):\n",
    "    ds['labels'] = ds['labels']*1\n",
    "    vals = ds['labels'].values\n",
    "    try:\n",
    "        vals = [item for sublist in vals for item in sublist]\n",
    "    except:\n",
    "        pass\n",
    "    labels = set(vals)\n",
    "    ds['num_labels'] = len(labels)\n",
    "    ds['label_set'] = labels\n",
    "    return ds\n",
    "\n",
    "def check_pre_norm(ds):\n",
    "    if ds['data_set'] in pre_norm_ds:\n",
    "        ds['pre_norm'] = 'Yes'\n",
    "    else:\n",
    "        ds['pre_norm'] = 'No'\n",
    "    return ds\n",
    "\n",
    "def convert_nan_to_val(data, value=0):\n",
    "    data[pd.isnull(data)] = value\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Use this if DOING a fresh modeling fitting analysis\n",
    "bn = True # use the percentile normalized data or no? \n",
    "log = False\n",
    "stand_scaler = False\n",
    "reanalysis = True\n",
    "model = 'log_reg' #log_reg, rf or svm\n",
    "\n",
    "if reanalysis:\n",
    "    pre_norm_ds = [ 'plasmaall_author',\n",
    "                    'urineall_author',\n",
    "                    'm_oxylipin_chronic_hep_b',\n",
    "                    'm_chronic_hep_b_POS',\n",
    "                    'm_chronic_hep_b_NEG',\n",
    "                    'm_CER_mass_spectrometry_v4',\n",
    "                    'm_CER_mass_spectrometry_v4_3_CS',\n",
    "                    'm_CER_mass_spectrometry_v4_0_NS',\n",
    "                    'm_CER_mass_spectrometry_v4_2_FS',\n",
    "                    'm_CER_mass_spectrometry_v4_1_COPD',\n",
    "                    'm_EICO_mass_spectrometry_v4',\n",
    "                    'm_EICO_mass_spectrometry_v4_3_CS',\n",
    "                    'm_EICO_mass_spectrometry_v4_0_NS',\n",
    "                    'm_EICO_mass_spectrometry_v4_2_FS',\n",
    "                    'm_EICO_mass_spectrometry_v4_1_COPD',\n",
    "                    'AN000580',\n",
    "                    'AN000581',\n",
    "                    'AN001503']\n",
    "\n",
    "    if bn:\n",
    "#         path = './bn_pickles/*.pkl'\n",
    "#         path = './bn_pickles/ST00006*_bn_data.pkl'\n",
    "        path = './bn_pickles/MTBLS72*.pkl'\n",
    "    else:\n",
    "        path = './pickles/*.pkl'\n",
    "\n",
    "    datasets = OrderedDict()\n",
    "    for fn in sorted(glob.glob(path)):\n",
    "        data = pd.read_pickle(open(fn,'rb'))\n",
    "        datasets[data[0]['study']] = data\n",
    "    \n",
    "else:\n",
    "    #### Use this if NOT doing a fresh modeling fitting analysis\n",
    "    pickle_file = './YES_bn_ds_models_and_sigfeat_NO_log_NO_standscal_NO_multi_mapped_labels.pkl'\n",
    "    ### The non-batch corrected pickle for the dataset\n",
    "    # pickle_file = './NO_bn_dataset_models_and_sigfeat_YES_log.pkl'\n",
    "    datasets = pickle.load(open(pickle_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X,y,ds,model):\n",
    "    X,y = shuffle(X,y)\n",
    "    if model == 'log_reg':\n",
    "        print('using logistic regression')\n",
    "        if ds['num_labels'] != 2:\n",
    "            clf = LogisticRegressionCV(scoring='accuracy', penalty='l1', solver='liblinear', tol=1e-4, intercept_scaling=1, max_iter=500, multi_class='ovr')\n",
    "        else:\n",
    "            clf = LogisticRegressionCV(scoring='roc_auc', penalty='l1', solver='liblinear', tol=1e-4, intercept_scaling=1, max_iter=500)\n",
    "    elif model == 'rf':\n",
    "        print('using random forests')\n",
    "        param_grid = {'n_estimators':[100,500,1000]}\n",
    "#         clf = RandomForestClassifier(n_estimators=1000, n_jobs=-1)\n",
    "        clf = GridSearchCV(RandomForestClassifier(n_estimators=1000, n_jobs=-1), param_grid, cv=3, n_jobs=-1)\n",
    "    elif model == 'svm':\n",
    "        print('using SVMs')\n",
    "#         clf = SVC(kernel='rbf', probability=True, C=0.000001)\n",
    "        param_grid = {'gamma': [1e-3, 0.01, 0.1, 1], 'C': [0.01, 0.1, 1, 10, 100]}\n",
    "        clf = GridSearchCV(SVC(kernel='linear', probability=True), param_grid, cv=3, n_jobs=-1)\n",
    "    else:\n",
    "        print('no valid classifier input, please try again with one of: log_reg, rf or svm')\n",
    "        exit(0)\n",
    "        \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=False) # so this will probably give rather high - at the end you just get the last model...\n",
    "    aucs = []\n",
    "    for train, test in cv.split(X,y):\n",
    "        x_train, y_train = X[train], y[train]\n",
    "        x_test, y_test = X[test], y[test]\n",
    "        if stand_scaler:\n",
    "            scaler = StandardScaler()\n",
    "            x_train = scaler.fit_transform(x_train)\n",
    "            x_test = scaler.transform(x_test)\n",
    "        clf.fit(x_train, y_train)\n",
    "        if ds['num_labels'] != 2:\n",
    "            if ovr_auc:\n",
    "                # to do one v the rest AUCs:\n",
    "                y_pred = clf.predict_proba(x_test)\n",
    "                num_labels = y_pred.shape[1]\n",
    "                set_to = num_labels+10\n",
    "                indiv_aucs = []\n",
    "                for ind in range(y_pred.shape[1]):\n",
    "                    y_mut = y_test.copy()\n",
    "                    y_mut[y_mut==ind] = set_to\n",
    "                    y_mut[y_mut!=set_to] = 0\n",
    "                    y_mut[y_mut==set_to] = 1\n",
    "                    fpr, tpr, _ = roc_curve(y_mut, y_pred[:,ind])\n",
    "                    auc_value = metrics.auc(fpr, tpr)\n",
    "                    indiv_aucs.append(auc_value)\n",
    "                aucs.append(indiv_aucs)\n",
    "            else: aucs.append(clf.score(x_test, y_test))\n",
    "        else:\n",
    "            y_pred = clf.predict_proba(x_test)\n",
    "#             print(y_test, y_pred)\n",
    "#             print(clf.predict(x_test))\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred[:,1])\n",
    "            auc_value = metrics.auc(fpr, tpr)\n",
    "            aucs.append(auc_value) \n",
    "#     x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "#     clf.fit(x_train, y_train)\n",
    "#     print(aucs)\n",
    "    auc = np.asarray(aucs)\n",
    "    if ds['num_labels'] != 2:\n",
    "        multi_aucs = auc\n",
    "    else: multi_aucs = 0\n",
    "    return auc.mean(), auc.std(), clf, y_train.shape, y_test.shape, multi_aucs\n",
    "\n",
    "def fit_model(X,y,ds,model):\n",
    "    mean, std, clf, train_size, test_size, multi_aucs =  train_model(X,y,ds,model)\n",
    "    if mean == 1.0 or mean == 0.5:\n",
    "        mean, std, clf, train_size, test_size, multi_aucs = train_model(X,y,ds,model)\n",
    "    return mean, std, train_size[0], test_size[0], clf, multi_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTBLS72 IPO_aligned_MTBLS72_neg (127, 6417)\n",
      "using logistic regression\n",
      "0.8519632414369257 0.055249500879693415\n",
      "using logistic regression\n",
      "0.8720969089390141 0.03058762366786248\n",
      "using logistic regression\n",
      "0.8154553049289893 0.08027156765043858\n",
      "using logistic regression\n",
      "0.8604427736006682 0.0400955511030673\n",
      "using logistic regression\n",
      "0.7816833751044276 0.11767525234528883\n",
      "using logistic regression\n",
      "0.836173767752715 0.03139608377386206\n",
      "using logistic regression\n",
      "0.8058688387635756 0.10206358984958748\n",
      "using logistic regression\n",
      "0.751106934001671 0.18573426354938558\n",
      "using logistic regression\n",
      "0.8388053467000836 0.06146860307687308\n",
      "using logistic regression\n",
      "0.8270258980785297 0.10186499524133383\n",
      "0.82406223893066 0.03507947282703626\n",
      "MTBLS72 IPO_aligned_MTBLS72_pos (127, 4529)\n",
      "using logistic regression\n",
      "0.7501670843776107 0.09068468999852028\n",
      "using logistic regression\n",
      "0.8889724310776943 0.020824304528210755\n",
      "using logistic regression\n",
      "0.8104636591478697 0.09767940668947135\n",
      "using logistic regression\n",
      "0.8288220551378446 0.11349857418440128\n",
      "using logistic regression\n",
      "0.7975355054302422 0.13349296771464433\n",
      "using logistic regression\n",
      "0.8517335004177109 0.043762892040722756\n",
      "using logistic regression\n",
      "0.7659983291562238 0.09882917351780293\n",
      "using logistic regression\n",
      "0.7945488721804512 0.12870689612775887\n",
      "using logistic regression\n",
      "0.8226608187134502 0.06428422147423468\n",
      "using logistic regression\n",
      "0.8261069340016709 0.08125992861819448\n",
      "0.8137009189640768 0.038112730305229575\n"
     ]
    }
   ],
   "source": [
    "# Used to fit models for all the datasets!\n",
    "for k, v in datasets.items():  \n",
    "    for ds in v: \n",
    "        print(k, ds['data_set'], ds['features'].shape)\n",
    "# if multiclass has not been pre-reduced to different one-v-rest datasets do you want to use one-v-rest or true multi class accuracy\n",
    "        ovr_auc = True\n",
    "# if working with batch corrected data, must make the labels and data into pd dfs...\n",
    "#         if bn:\n",
    "#             ds['labels'] = pd.DataFrame(ds['labels'])\n",
    "#             ds['features'] = pd.DataFrame(ds['features'])\n",
    "        ds = get_num_labels(ds)\n",
    "        ds = check_pre_norm(ds)                \n",
    "        y = ds['labels'].values.copy().ravel().astype(int)\n",
    "        X = ds['features'].values.copy()\n",
    "        X = convert_nan_to_val(X, value=0)\n",
    "        X[np.isinf(X)] = 0\n",
    "        X[X<0] = 0\n",
    "        if log and ds['pre_norm'] == 'No':\n",
    "            print('using log')\n",
    "            X[X<1] = 1\n",
    "            X = np.log2(X)\n",
    "        aucs = []\n",
    "#         X,y = shuffle(X,y)\n",
    "        for i in range(10):\n",
    "            auc, auc_std, train_size, test_size, clf, multi_aucs = fit_model(X,y,ds,model)\n",
    "            print(auc, auc_std)\n",
    "            aucs.append(auc)\n",
    "        aucs = np.asarray(aucs)\n",
    "        print(aucs.mean(), aucs.std())\n",
    "#         ds['auc'], ds['auc_std'], ds['train_size'], ds['test_size'], ds['clf'], ds['multi_aucs'] = fit_model(X, y, ds, model=model)     \n",
    "#         print(ds['auc'],ds['auc_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
